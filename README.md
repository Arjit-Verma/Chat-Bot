Made this chatbot using Ollama and meta llama3.2 model with 1 billion paramter this runs locally on default port 11434
Using this we can get reponse from it using "POST" method call.

Steps to follow:
download ollama.
download the specific model you.

using the backend and frontend package in here we can use any model,so change the model name in server.js and run the backend and frontend package.
Do npm install in backend and frontend .

To run backend use "node server.js"
To run frontend use "npm run dev"

That's it you are good to go.
